My name is Dara Denny and welcome to our new series, Ad Spend. This series isn't about getting you to spend more money on your advertising. What we're actually going to be doing is going behind the scenes with real brands who are using meta ads to scale, breaking down their creative, their growth strategies, and everything that's working today in paid social.

Today, we're speaking with the probiotics brand Seed Health about the unique way they approach creative testing. We're going to talk to their growth manager and their creative strategist all about this unique methodology that they have and really break down how they are structuring their tests inside of Ads Manager. I am so pumped to have this conversation, so let's get started.

Now, I really love Seed's creative strategy because they are not afraid to say the bold thing in their ads. The brand was founded in 2016 with a mission to bring more science to the wellness space, which was often filled with misinformation and vague claims. Using meta ads as their main marketing channel, Seed began to scale super rapidly, spending seven figures a month on the platform alone.

However, that scale and success brought them a challenging problem. They were spending a huge portion of their budget and resources on creative testing, which honestly wasn't super profitable for them. They had a nagging feeling that even though they were following best practices, it might not actually be best for their business in 2025.

So here's what their creative testing methodology used to look like. They refer to it as sandbox testing. So initially, we were doing what I think most advertisers were doing.

We were sandbox creative testing, where we had your typical creative testing campaign. We had concepts, and every single concept was a different ad set. And we were pushing and forcing spend to every single ad so that we could really empower Kristen as the creative strategist, as well as our brand team, with all of the information and data on every single ad so that they could make informed decisions on how to move forward.

While the strategy is a great way to test a wide variety of ads to find winning ad creative, Seed was really questioning whether or not this was actually right for them at this budget. They felt that the meta ads algorithm actually did a pretty good job of determining whether or not an ad creative was going to be scalable or not. However, underneath sandbox testing, they were actually throttling spend pretty equally to creatives initially.

They felt like they were perhaps pushing that spend towards creatives that the algorithm already knew weren't super scalable. So with the help of their meta disruptor partner, they actually brainstormed a brand new methodology using ASC to test their creatives, which ended up actually giving them way better results while ironically testing less creatives. It is about 15 to 20 ads, could be about three to five concepts, depending on how many ads are in each concept.

And we group them all together, and we allow the algorithm to decide where does the spend go. And that's what we use as our number one lever of success on an ad. Does it actually scale?

And most importantly, does it scale at efficiency? But even if it doesn't scale at efficiency, the fact that the algorithm really pushed that ad and put a lot of spend to it means something to us more than it did before when we were forcing spend. Seed has their own internal creative team that focuses on graphics, high production creatives, and whitelisted content with influencers.

But they also have three creative agencies that they work with. Two of them focus on UGC, and the third on graphic design statics and AI creative. They typically launch four to five creative tests or sprints per month, which are divided up by creative source and can have anywhere from 12 to 25 ad creatives inside any given sprint.

The other interesting thing about ASC testing for us is that before when we were sandbox testing, we really had to feel like we were beholden to four to five variations of a concept. Now, if we have one concept that may be worked on another platform and we want to throw it into an ASC creative sprint, we don't need to have five variations. So I think it's allowed us to be a lot more flexible, actually, which is an interesting outcome of ASC creative testing.

So what Seed is essentially saying here is that sometimes they can actually test more variations of a certain concept if they have a lot of conviction that that's actually going to do better. Or they can actually test less variations and really just double down on the creative variable that they have the most confidence in. That is, instead of arbitrarily just coming up with variations for variation's sake, they can actually just test the creatives that they think actually have the most likelihood of being scalable.

And something that's really interesting to note is I've actually worked on teams that were spending a similar amount to Seed, but they were actually testing two to three times more creative because of that variation testing methodology, even if they didn't have the highest confidence in some of those variations that they were testing. What I have found, and I don't know if this is validated by Meta or not, but visual differentiation is the bigger unlock than like iterative copy testing, where I have found that launching like slight tweaks on copy is not a big enough lever to actually get a different result there. The way I think about it is like if you're looking at something and in two seconds you can't tell that it's different, the personified algorithm may not know it's different either.

Now, this is a really interesting point because to what I said earlier, a lot of times in sandbox creative testing, you are going to be doing variant testing, which is often just testing different messaging points or different hooks, which makes me wonder actually if this bigger swing ASC style of testing is actually going to be more commonplace the further we get into 2025. Actually, let me know in the comments how you guys are currently conducting your creative testing and if you are interested in giving ASC testing a try. It is a careful balance between like variations for variation's sake and the quality of how differentiated your creative is.

As an example, Kristen shared their biggest creative learning to date and how they've been able to diversify and extend the life of that creative concept. So what is that top performing creative? It's the aspirational pooping every day.

Will I poop when I'm traveling? Will I go before the big game on this big day? And I mean, it's our top value proposition on Meta.

They've been able to explore this concept and add true creative diversity to this learning by leaning into the specialties of their internal and external teams. The ones with the static pooping every day are coming from our internal creative team where we have a really great tight feedback loop on briefing, copy review, brainstorming within our growth ads pod. These learnings are then passed down to their creative agencies where they then explore this concept within their own unique specialty.

And then the UGC assets are actually from two of the creative agencies we're working with. And that has been really interesting to take learnings that have performed well for us from an internal creative work stream standpoint and brief those into an outside agency and someone who doesn't live and breathe seed all the time to think about how they would actually bring that concept to life visually and in terms of a storyline and a script. So it's been really interesting to take a learning that we have pretty high conviction in and explore that through different creative sources and see how different creative sources with different creative teams and different expertise will bring that to life in a way that provides us success and diversity within the account.

Now what this says to me is you don't always need your agencies to come up with net new concepts. And you don't always need to pursue volume for volume's sake. In this case, Seed basically know what's working and they want to be able to lean into their external partners to then test it out in their own specialty formats.

This is another way that Seed's ASC creative testing methodology really allows them to stay laser focused. We could put 50 ads in an ASC, but we know that only three to five of them are going to get prioritized. So when we work with these agencies, we actually keep a pretty tight cap on the amount of variations we get because we want to make sure we're getting learnings on the things that we feel the most conviction in and we're not like going super wide and putting a ton of resources behind something that is unproven.

By and large, the thing that works the best for Seed, unsurprisingly or not, is their whitelisting content. And depending on their performance at any given time, they know that they can lean into their whitelisted influencer content if they need a performance boost. However, on the flip side, if they have really excellent results, they know that they can test out more of those bigger swings or unproven concepts.

However, a really important nuance that they shared was actually a learning that they had based on an incrementality test with Meta. For us, what is most efficient for an internal attribution is our whitelisted ads. And so with that, obviously, as a media buyer focused on efficiency, you're going to start shuffling a lot of your money towards those.

But you start to question a little bit of, you know, what do these other sources bring to the table that we're not currently seeing in our internal attribution? And so we actually did an internal study looking at the incrementality of our whitelisted influencer ads versus our internally made creative. And what we found is they actually have very similar incrementality.

Just the last touch attribution is completely different. And really where those conversions are coming in from our internally made creative is from view through, which means for us on our internal attribution, people are still purchasing. They're just purchasing, maybe not last touch on Meta.

I could push all of my budget to whitelisting ads that's our most efficient, but would there be overall higher tax across other channels? And so that's where the question starts to come in of like, how do we keep diversifying sources and keep bringing different people into our funnel, even if it's sometimes like not the perfect thing for internal attribution on that individual channel and what's better for the greater good of the business? This was honestly a huge light bulb moment for me.

As media buyers, we are trained to optimize ad accounts by cutting out what isn't performing as well and doubling and tripling down on where we see performance. But Tiffany is essentially saying that this can be short-sighted. So in this case, whitelisted influencer content can often lead more to immediate purchases, while some of their other content might be really great for generating that top of funnel touch point and then eventually lead to a purchase, but maybe not within the same timeframe as that whitelisting content.

And when their team isn't in a performance crisis, they actually continue to spend on these diversified assets, even if the results don't look as great inside meta-reporting. And that's because they know through these lift tests that the ROI will be there through their business down the line. And I think one of the really interesting things that came out of the incrementality study is the most efficient incremental CPA was the holdout sell, where we were running a blend of creative from our internal team, the whitelisted program, and the agencies we're working with.

And I think that is sometimes hard, unless you're looking at the overall health of the channel over time, to see the impact of running diversified creative. And to me, that was a huge wake-up call that, yes, we are being held to these very strict last-touch CPA goals. And Tiffany has done so much amazing work at reducing the last-touch CPA for meta.

But there are sometimes actions happening within a marketing channel that are hard to derive in such a narrow way. And that, to me, was just a big flag that we are doing what we need to do for the overall health of our account, which is continuing to run diversified creative, even though not everything has the same last-touch CAC. Now, this is really fascinating to me because essentially what Kristen is saying is that the most effective campaign was not the whitelisting campaign that had the best tracked results.

It was actually the campaign that had the diversified assets underneath that single campaign. Now, this makes me wonder if the next era of testing meta ads creative is actually going to be more like developing a creative ecosystem, almost like a 360 marketing campaign where there are going to be multiple formats, multiple talent options, multiple levels of awareness, and just multiple ways to actually reach your target customer. I like to think of it more as like a curation.

From the outside looking in on this process, it is really clear to me that Kristen and Tiffany are a true force and a lot of their success actually boils down to their personal relationship and how much trust they have in one another and also how their team is structured. The change that we made was really afforded by the fact that my function sits within growth and we share these goals of growing the business sustainably and efficiently. And I don't think it would have worked if you were trying to make this case with someone who is planning your creative and was on a completely separate team.

So I think that's a little bit of the unique organizational structure here and the fact that like Tiffany and I are working together on a daily basis and while we need to think about how we position learnings and how we change that narrative to the creative team as a result of this, I said to Tiffany, if this helps us reduce our CAC and we can actually test more because we've produced our CAC, that is a win for the business. And for me, this is what the future of media buying and creative strategy actually look like. It's all about growing businesses and building teams that make that a lot more effective.

Final question is, if you could give one piece of advice to a brand just starting out, what would you say? Track what you're testing, track what you're learning. I think like creative testing on Meta has the ability to be the way an entire brand learns and that's really what we have found is that like that's where we're doing the bulk of our creative testing and having documents that I can shoot off to the organic social team with our top performing ads.

Like everyone wants to know what's working well on Meta. And then I think for me, it's like when looking at data and performance, look at the entire picture, not just a single metric, like really take into consideration every piece of the funnel and every little, the click-through rate, the CPMs, really every piece of it. As you kind of devise creative performance and everything, you can really get a full funnel picture there and be able to make better hypotheses and stuff for the future of how to iterate.

And that concludes our first episode of Ad Spend. I cannot wait to hear what you guys think. Since having this conversation with Seed, I have been thinking nonstop about what this next era of creative testing is going to look like on Meta Ads.

Are we really going to be rolling out sandbox testing? Are we going to be developing creative ecosystems inside of our ad counts for our businesses to thrive? Who knows?

But I am so curious to hear what you guys are going to be testing during this next season. And I will see you all next week. Love you.

Bye. And of course, a huge thanks to Meta for partnering with me on this video.