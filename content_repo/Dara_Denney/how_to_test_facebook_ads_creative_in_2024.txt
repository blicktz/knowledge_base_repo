What's up marketers? I'm gonna show you how to test your Facebook ads creative in 2024. I'm gonna show you the exact framework that the brands that I work with use to test thousands of ad creatives every single month.

So let's break down exactly what we're gonna be learning today. Number one, I'm gonna show you exactly how to structure these tests inside of Ads Manager and how, when, and if I'm using Advantage Plus. I'm also gonna show you how to strategize and develop an effective creative test.

This happens before you get an Ads Manager. I'm also gonna talk about which creatives you should test first or next and how I balance iterations versus net new testing. There's gonna be a ton to go over in this video.

So let's go ahead and hop inside of Ads Manager and I can show you exactly how this test is structured. So first we're gonna start off with a really quick account structure overview. Now I am going to be updating my account structures for 2024, but the real basic structure is you're seeing one Advantage Plus shopping campaign in there.

And then you're also seeing a testing campaign in there. Now this has not changed since I created my last creative testing video. For every single creative test, let's go ahead and hop in here.

You are going to be actually creating a new ad set for every single creative test. So you can see in here, we got test one. This is a features point out image.

We have test two in us versus them. Let's go ahead and hop in this test really quickly and what it looks like. Oh, okay.

We have a few different ads in there. We have up to three versions and we can also see, okay, there's a little test description of how we are actually making these variants. A better example of this would be something like, okay, for this us versus them, we're going to be comparing against a generic industry.

Maybe we'll include a customer quote, or also be comparing that against a direct competitor. Now, a few big questions to answer about this structure. Number one, why should you segment your creative test versus your other campaigns?

Number one, this is to protect performance. What you'll see when you are adding new creatives into ad sets or campaigns that you already have running, that is going to affect the performance. Sometimes it'll be positive, sometimes it'll be negative.

You don't want to unintentionally tank the performance of an ad set or a campaign that's working well by adding in creative that doesn't actually end up being effective. Also, you want to make sure that your test actually gets spent. A lot of times, what's really common is if you take a creative and add it into an already performing ad set or even an ASC campaign, you'll see that your new creative will not get any spend, so you won't get any learnings from it.

So that's why we keep it in its own campaign and intentionally throttle money into it. And number two, for me personally, it's just way easier to read and digest data. And it's also really easy to break down, hey, how much did we spend on testing this month versus our business as usual campaigns?

And finally, it is easier to scale this way, in my experience, because you will be, in essence, scaling in a few different places. I call this horizontal scaling, right? You're not just scaling vertically with the budgets, but if you are able to scale up a specific creative inside its own campaign and inside its own asset, while also dripping that creative into the ASC Advantage Plus shopping campaign, or also putting it into your other campaigns if you have those going, that is also another reason why I like having both those there, because you're also able to reach new audiences too.

Inside of the creative testing campaign, I am primarily testing on a completely broad audience, but maybe for some of my other brands, they have other campaigns where they're using some targeting, not as common these days, but they definitely have the ASC campaigns. And because of the nature of the ads that are inside of those campaigns are different, you are reaching different audiences. And number two, just to underscore this, all of the targeting inside of the creative testing campaign is broad.

The reason why we do this is because broad is the most scalable audience. And also if you have other targeting inside of your Facebook ads manager, broad is also more likely to work on those other audiences. And honestly, this is not something that I fret over very much.

Even for brands that are spending as little as $5,000 per month, they are also testing their creatives on broad. Now, another thing I wanna talk about really quickly are the variations of ads. So when we hopped into the creative testing campaign, you saw the different ad sets and we went to see how many ads were inside of that creative testing campaign.

You could see that there were three variations for most of them. I am generally recommending brands now do two to four variations of every single creative test. And one of the differences I've seen this year is that these variations need to be much bigger swings in order for you to get the best chance of actually getting a winning creative.

More on that in the next section about how to actually develop an effective test. Another reason why I like to add variations into every single creative test is this bakes in iterative testing, even into net new concepts, which means no matter what, even if a creative doesn't ultimately do that well for you, you're able to walk away with concrete data from every single test, regardless of the results. You can see, oh, this one had a better click-through rate.

It was better at actually driving traffic to the website, or it had higher hook rates. So we were able to see, oh, this one did a much better job at getting attention. And you can document those learnings from every single test.

Now, another thing I want to talk about is the budget. How much should you budget for these creative tests? Now, generally, I work with brands that are spending more than 100K per month on their Facebook ads.

And I almost always start my creative tests at $200 a day. This is going to help most brands get out of the learning phase or learning limited in a good amount of time. However, I know that that's not super accessible for a lot of brands.

What I will say is that the days of the micro budgets, like the $5 ad set, those are totally over. That does not work at all anymore in 2024. But I have still seen success for brands trying their testing at $50 today or up to 100.

So that's really the absolute minimum, I'd say, for a creative test. And ultimately, you're going to want to let that run for about seven days. And sometimes I'm getting really early reads on a creative.

And if I see that it's doing really, really well, after three to four days, I'll go ahead and start to scale it up 20% to pressure test it and see if we can get a little more scale out of it in place. Or if it totally flunks and all metrics are bad after three, four days, I just go ahead and turn it off. I don't stress about it.

Another big question I get asked is, should you use ABO or should you use CBO? This is honestly a media buyer's dealer's choice. It really depends on the media buyer.

I have seen both strategies work really, really well. But I will say, if you're using CBO, you really need to make sure that you are adding ad set minimums and maximums so that you can make sure that you're throttling certain spends through there. I personally use ABO because I think it's a lot easier to get the read on what's working if I am manually increasing and decreasing the budgets.

But that's just because I'm a bit of an old school media buyer. And I like to have the pulse acutely on the performance of a creative test. But a lot of people would disagree with me.

Charlie, the disruptor. Now let's talk about how to strategize and develop a great creative test. Now, if you're going to be developing a creative test, the first thing you really need to do is look at the big picture of what you want to be testing.

So are you going to be testing a new format? Are you going to be conducting a messaging test? Are you going to be trying out a new angle or potentially a new hook or a new creator?

I find that most of the time people want to try out a new format, right? So, oh, maybe they've tried a Features Point Out or an Us versus Them, but now they want to try a Founders ad and they want to try a bullet point style ad, something like that. So once you decide, okay, this is the overarching type of tasks that I want to conduct, then you're going to need to do two things.

You're going to need to first come up with a hypothesis. And essentially what the hypothesis is going to answer is why do you want to make this test? And why do you think it's a good idea?

I'm going to show you an example in a second. The second thing that you're going to want to consider are the variables that you want to test or the variations that you want to test. A lot of times this ends up being a messaging test, but more and more, since we're taking bigger swings, it's going to be a combination of messaging and imagery tests.

Sometimes the variations look very, very different. There's a few reasons why you should always come up with a hypothesis before you make a creative test. Number one, it's going to make you a much more critical thinker, and it's going to allow you to prioritize which creative tests you should be doing first, versus which ones maybe you should develop more over the longer period.

So here's what it actually looks like in my reporting, right? This is an example of a creative test where we had the testing criteria. So the hypothesis here was, hey, maybe if we reference TikTok in Twitch, we will drive an increase in sales.

Now this isn't actually a good one. What a much better one would be, would be also giving insight to why this would be a creative test. I think that this is a good creative test because we're seeing a lot of our users mentioning TikTok in Twitch on their other organic communities, or this seems to be a trend that's working more broadly on the platform, and we want to see if it can work with us, something like that.

And then of course you can see the variations and variables right there too. Back to the idea of having bigger swings in your iterations or your variations, these bigger swing tests seem to have much better results. So when you were thinking about the different variations that you want to test, especially if you're just starting out, you don't want to do as many one variable testing.

Again, unless you're a more experienced brand and there's a very specific type of learning that you're after. If you're going after a pure performance play, your variations should honestly be as different as possible so that you have the best chance of actually reaching a pocket of users that ends up with a creative that is more scalable. In the end, it's mostly about creating uniquely high potential ad creatives.

Especially in the beginning, you're going to be able to get messaging learnings from your tests, especially as you do more and more. In those same light bulb messaging learnings that you could get from split testing things really concretely, you're going to learn those things anyways after a longer period of time. And I'm going to show you a few examples of this towards the end.

Yeah. So it's interesting. Ma and Bo is doing a test right here.

These are the really typical super split testing. They are only split testing the messaging. Again, they're a bigger brand, so they probably have something really concrete to learn here.

I generally wouldn't recommend this for brands starting off or for most brands really, again, unless you want to have a super concrete learning. I actually really like the other tests that they're doing here where the background and the setting is different. And they also have a different testimonial with each one, which I actually think is giving them a much better opportunity to succeed.

And since they already have so many creative tests and learnings in the mix, they're probably going to be able to extract a learning from that test while also having the most potential for success. Another example of a really good creative test is this Crocs example right here, right? So they wanted to test some nostalgia, Microsoft Paint style-esque marketing that is targeting both more Gen Z as well as millennial, right?

Because of that nod to Microsoft Paint. And what I think is really cool here is they could have just split tested the language, but since they were able to do honestly really different formats within the same strategy, they were able to position themselves for a higher chance of success. Okay.

So what creatives should you test first or what creatives should you test next? Generally, when I first start working with a brand or if a brand is brand new, I like to conduct what I call rapid format testing. And this is where I generally find the most success.

And what that means is if we hop back into this ad account, you can see, oh, she's conducting a whole bunch of different format tests here. It's not really a messaging test, even though that is happening in the different variations in the ad level. Now, the reason why I conduct rapid format testing is because the new hook rate is more like one second.

So how your ad actually just shows up on someone's phone is actually going to be a pretty big determination if they're going to engage with it or not. So the way that I prioritize, you know, deciding which formats to test first is number one, is there any low hanging fruit? What haven't they tested yet?

That seems like it could be a potentially big win or is there a certain format that I've seen work across the board that they haven't tried? That's definitely something that I would have them try them. And then it's going to be a combination of, hey, what do I think is going to have the biggest performance gains?

And also what is going to take the shortest amount of time to make? Because some things are going to take more time. So first, I'm going to prioritize everything by potential performance gains.

And then I'm going to reorganize it by, okay, what can we actually execute on in the shortest amount of time? Sure, a founder's ad might be a huge unlock for a brand, especially if it's a huge studio shoot and they're investing a lot of time and money into it. But most brands can't do that tomorrow.

They could do an ugly ad tomorrow and get really great results out of that. So I really like to prioritize based on a confluence of performance and time. And for most brands, I'm making sure that they've tested out these formats that I see convert again and again across industries.

So things like a features benefit point out, us versus them, before and afters, problem solution, UGC, founders ads, testimonial ads, statistic ads, ugly ads. Now let's talk about iterations. So going back to our creative testing campaign.

So let's say that we conducted a few creative tests and we're starting to see results and we have a few winners and a few things that didn't work so well. The first thing you're going to do is you're going to analyze the tests on an individual level. Why do you think that this specific creative won out over this creative?

Generally, there's going to be one to two really clear winners. Very, very seldom do I see all variations win. So I think a really good example of this is like this array bullet point ad.

Okay. So we're looking at this ad and let's say that the de-bloat in under one hour won. Okay.

Interesting. We are seeing that this specific headline worked for them. Cool.

Now what you're going to do is you're going to go through the rest of your creative tests and you're going to do that and extract learnings across all of them. And then you're going to take a step back and see if there are similarities across all of the winning ad creative. Was there anything similar in the messaging, in the imagery, in the creator, in the angle, or even the format that could ultimately signal a larger learning?

So say we take a look at the bullet point ad, right, along with a winning ugly ad and a winning UGC ad and we look at these together and say, okay, is there a larger learning here? And interestingly enough, there is. A learning that we can see from this is actually within the messaging, which is, Hey, using the phrase deep loading in under one hour seems to really increase conversions for us, probably because number one, it's solving a problem, but it's also giving a timeframe for that problem.

And that timeframe is really, really attractive. Just one hour. That's awesome.

So having that learning and knowing that that type of message is resonating within a certain audience, I would then think, okay, what other types of formats can we spin up quickly and effectively? That could also increase conversions, something like a headline style ad, or even a GIF ad that has a little bit more movement and longer term as we're also creating more creator content and developing bigger content across platforms, maybe even across TV, across whatever. How could we include that learning so that we can not only continue building off of it, but also continuing to pressure test it and to challenge our assumptions about that learning.

Now, another thing too, when you're thinking about iterations is, you know, if you're on multi-product business and you see, Oh, this specific format works for this product, that's another iteration you should make. You should do it for all your other products. Again, this is something that Array is doing really, really well.

You know, I wish that I could tell you guys, Oh yeah, you know, test these 10 creatives and then do messaging testing. And it's, it's super different for every single brand. And the beauty of it is, is that every single strategist is going to approach it in a little bit different way.

But I really believe that by starting off with the format and then taking those learnings, those broader learnings that you see repeating as you do more and more tests and enveloping those into your larger strategy, that's where I tend to see the most success. But again, you need to continuously challenge those learnings, which is why you do iteration testing within every single net new concept. So even though we saw, Hey, this deep blow in under one hour is working, we're going to test that with a really new statement or a new golden nugget thought or idea or review from our users.

This is also why research and going over your comments and your reviews over the longterm. And as a longterm strategy is really important because as long as you keep building off of what your customers are telling you and what your customers are reacting to, AKA the data, that's how you're going to continually build your creative strategy. And that's honestly the only way that you should be testing your creatives in 2024.

If you guys liked this video, please give me a like and subscribe. Those small things really, really do help me out with me and my channel. I love you.

I'll see you next week.