What's up marketers? Welcome or welcome back to my channel. My name is Dara and I lead a growth team at the performance marketing agency thesis testing, where I work with direct consumer brands who are spending anywhere from a quarter million to $2 million per month on their paid social advertising platforms.

So things like Facebook ads, Instagram ads, Snapchat ads, TikTok ads, all the ads platforms. Now today's video has been highly requested by you. And to be honest, I feel like I might actually be revealing too much in this one.

I'm going to show you exactly how the paid social teams at thesis conduct creative testing inside of a Facebook ads account. And if you've seen my channel before, then you already know that creative is the most important lever that you can pull on the platform. This is why consistent, I might even say ruthless creative testing is the only way that you're going to find success on the platform in 2021.

But how does one actually test new creative in an ad account? Can you simply develop a new creative asset every few weeks and drop it into your current campaigns? Spoiler alert, no.

Additionally, how do you protect your campaigns from creative flops? Because there will be many. So at thesis, we've actually developed a creative testing methodology that helps you find more learnings quicker while also protecting your core campaigns from bad performing creative.

So no more fluff. Let's go ahead and dive in. The first thing I want to address is your overall account structure.

And yes, you should have a completely separate campaign to conduct your creative tests and like only your creative tests. So here's actually an example of the bare bones account structure that we use for our clients at thesis. Some brands might have DPA campaigns or even other campaigns for different geographical locations, but this is the core campaign structure that we use.

So the foundation of our creative testing methodology is that we isolate all the creative testing into its own campaign, which you can see here, T underscore prospecting testing underscore USA. And there are three main reasons why we separate our creative testing from the core campaigns. Number one, this ensures that the creative tests won't bring down the performance of the core campaigns.

By keeping the core prospecting and retargeting campaigns full of only our top performing creative, we have a better foundation for overall performance and even scaling. Number two, we're able to facilitate iterative testing even with net new assets, which increases the number of learnings that we get from every single test. And number three, learnings are simply easier to track and report on.

Now there's some other things to know about this structure. From a high level, we take the winners out of our creative testing campaign and put them into our core prospecting and retargeting campaigns. But more about this later.

Additionally, we typically don't use Facebook's A-B testing tool. We found that this actually tends to increase costs of creative testing with little to no upside. And the reason why that is because actually inside of our creative test, we're launching a new ad set for every single test.

And if we were going to use Facebook's A-B testing tool, we would be doubling the amount of those ad sets essentially. Some brands tend to get hung up on diversifying their creative between their prospecting and retargeting campaigns. This isn't actually something that I've seen much proof of because if something is performing well in prospecting, oftentimes it's going to perform just as well in retargeting.

However, sometimes if a creative test doesn't perform well in prospecting and we think that it actually would do well in retargeting, we'll just simply make the gut call to take that creative and put it into the retargeting campaign only to see how it does. And finally, because the core prospecting campaign, T underscore prospecting underscore USA, is always full of our top performing creative, this is actually where we conduct our audience testing with lookalikes and different types of interest segments. Now let's go ahead and jump inside the creative testing campaign.

So inside the creative testing campaign, each new creative test is separated at the ad set level. So essentially, each creative test gets its own ad set. We keep track of the test by assigning them a unique number, which is simply in chronological order so that we can actually show how many tests we've run for the client and a unique name, ideally one that details what the test is.

It's important to note that the only thing that is being tested in the creative testing campaign is creative. So things that are on the actual ad unit itself, we're not going to be testing things like bidding strategies, budget or scaling strategies, or even audiences inside of this campaign. And in most cases, each new creative test is going to be testing only one element of the creative unit so that we can isolate the learnings.

This includes things like ad formats. So your single image, your video, your carousels, your collection ads, your slideshow ads, gif ads, whatever. Also things like net new creative assets like videos or even images, thumbnails, copy like your primary text, your headlines, or even your descriptions, and then CTAs.

Another important call out at the ad set level is we conduct all of our creative tests with a broad audience. So these are ad sets with zero lookalike or interest targeting. Perhaps there's going to be a few guardrails on age, gender, or even geographical location.

But for the most part, this is an ad set that is targeting all of Facebook. And if you want to do a deep dive on the reason why we commonly use broad audiences inside of Facebook ads manager, be sure to check out this video above. But to give you the CliffsNotes version, it's ultimately the creative that drives the targeting and the best results on Facebook ads.

But an additional reason why we always use broad targeting to conduct these creative tests is because we want to create the most scalable creatives possible. Since broad audiences are the most scalable and often the cheapest, we know that when we find a creative that wins with broad in our creative testing, that we're going to be able to drip that into our core campaigns. And generally, those creatives are going to perform just fine, regardless of the additional targeting that we have there.

Now, let's go ahead and pop into these creative testing ad sets. So the first thing I want to call out is that inside of each creative test, we aim to have anywhere from three to six ads. And frankly, I want to test as many things as possible.

So I'm going to be gunning for that six. Six is still the number of live ads that we find optimal inside of ad sets today. However, I've lately been thinking that this is something that we should probably test into more.

Now, like I said earlier, another major perk of testing your creatives in this way is you get to right away jump into iterative testing. And as you can see here, we're actually testing six different variants of a single creative asset. In fact, under our methodology, all creative tests fall into one of two categories.

One is net new. So this is an image or a video that has never been tested before. And number two is iterative.

So once a creative winner has been identified, we actually like to conduct further iterative tests on that ad unit so that we can increase the lifespan of that creative. This article by my colleague, Lazer, actually breaks down 20 iterative tests that you can conduct on your creatives right now. This can include things like thumbnail tests, messaging tests, copy tests, CTA tests, and a whole lot more.

So from my perspective, an ideal test for a net new video would look something like this. Number one, of course, we would create a new ad set for the test. And we'd take the new video asset and decide what type of element we'd like to test first.

And oftentimes what we do for completely untested footage will actually do hook testing on the first three seconds of that footage. So essentially, we'll have the same asset, but the first three seconds of each of the videos are going to be different. Another thing to note is while we do test dynamic creative inside of these creative testing campaigns from time to time, we actually like to keep the copy, the headlines, the descriptions in the CTAs uniform so that we can get statistically significant results.

And again, the only element that is different here amongst all the different creatives is that initial hook. It's a similar story for using the multiple text options at the ad level. Since Facebook doesn't actually report on the type of copy that is doing best or really any data about that at all, we actually like to conduct our copy tests into their own ad set tests so that we can actually have that data on what is performing best.

However, if we do find a number of copy options that work, sometimes inside of my core campaigns, I'll actually launch an ad that has a multiple text option. And once that creative test is set up, that's when we launch. Now the next question that we often get at this point is how much do you budget for creative testing?

So according to Facebook's best practices, you're actually going to need 50 optimization events in a single week to exit the learning phase. And until you exit the learning phase, Facebook says that you're going to have higher costs and often poor results. So if you wanted to follow Facebook's best practices to a T, then you're going to need to get 50 optimization events into each of your creative tests before you even evaluate it.

So under these guidelines, if you had an average CPA of like 35, then you would need at least $1,750 per week to run this creative test, which would boil down to a budget of $250 a day. And in practice, I would say that thesis is pretty close to these targets, but we really aren't militant about the 50 optimization events, especially if we see a test performing poorly. Most times we'll use the CPA targets to get a rough idea of the initial daily budget, but then we'll optimize based on performance after a few days of spend.

An additional consideration here concerning budget is what percentage overall you want to have your creative testing campaigns versus your core campaigns and prospecting and retargeting. And to be honest, every single brand is different in this regard. Some brands actually find their best performance coming out of the creative testing campaign.

So as a result, they actually have a higher percentage in testing as opposed to their core prospecting and retargeting campaigns. I'd say the broad strokes average across our clientele at thesis is some of our clients have only 15% of total ad spend in their creative testing campaigns, while some of them have up to 65%. Ultimately, we typically recommend our clients to follow where the best performance is.

So if we're seeing better results coming out of that creative testing program, then that's exactly what we're going to do. However, at a starting point, I think I would recommend something around 20% to be dedicated to creative testing per month. Okay, so you've launched your creative test, you've set that budget, what actually happens after you launch that campaign?

So once a creative test is launched, we typically don't start analyzing the results until at least three days of total spend have gone through the ad set. This allows the ad set to distribute the budget amongst creative and you should start to get a sense of what kind of CPAs are actually coming out of the creative tests. And here we often find ourselves in one of three scenarios.

Scenario number one, which is the best is that results are really good. CPA is lower than average, and there are at least one or two potential creatives in the ad set. Number two, which happens often, is that the results are average.

So there are a few purchases being generated by the new creative tests, and they're following anywhere between 10 to 25% of the normal CPA targets in the account. And number three, also pretty common, is that the results aren't good at all. CPAs are high across the board, and there are little to no purchases being generated from the new creative tests.

So let's go back to scenario one. When the results are looking really good in that new creative test, we actually start scaling up that ad set directly in the creative testing campaign. We actually do this before we start taking that new performing creative and dripping that into our core prospecting and retargeting campaigns.

And we do this because we want to make sure that this new creative can withstand an increased budget. And again, according to Facebook's best practices, you'd want to have that increase be about 20% every three days so that you can avoid the learning phase. However, depending on the temperament of the account, sometimes we actually like to increase that budget anywhere from 50 to even 100%.

And this is so that we can drive those learnings faster. Typically, we'll actually do this anywhere between two to three times before identifying a creative winner, at which point then we will start duplicating it into our core prospecting and retargeting campaigns. Now I want to mention something that's really important about this process, which is that we never ever turn off an ad set that is getting good results.

Even if we've already taken that winning creative and duplicated it into our core prospecting and retargeting campaigns, we will actually keep that ad set going for as long as the results are strong. And the reason why we do this is number one, this is a great way to scale. And number two, sometimes when you actually take that winning creative and put it into your core prospecting and retargeting campaigns, it doesn't perform as well because it is up against other top performers.

But as results start to dwindle on this ad set, or even for the creative tests that didn't have as good performance, these are the steps to optimization that we take. Number one, we always start optimizing at the ad level. For instance, if we see that an individual ad has a high CPA or per performance after a few days of initial ad spend, we'll actually go ahead and turn it off.

And this is so that we can allow other ads to get that spent. And we'll repeat this process every few days until we can identify the CPAs for each ad. And if an ad reaches about 2x, the average CPA in an account, we'll actually just go ahead and turn it off.

And if no winners are found in a creative test after five to seven days, then we'll go ahead and turn off that ad set. Another common question we get is which metrics do we use to actually determine creative winners? And we use a combination of primary and secondary KPIs to actually tell the story of an ad's performance.

And since we are a performance growth agency, revenue is our number one priority. So the most important primary KPIs that we use to evaluate an ad's performance is CPA and the amount spent. What we're really looking for is an ad that can get as much spend through it as possible while still maintaining a low CPA.

Historically, we also use cost per ad to cart as an early indicator of how an ad would be performing, but this has become less clear after iOS 14. But CPA and the amount spent only really saying if the ad worked or not. They don't tell the story about why an ad performed well, or often more importantly, why it didn't.

So to tell that story, we also track these secondary metrics to inform our learnings and also create better iterations in the future. The first metric is a hook rate. Now this determines the percentage of people who watch the first three seconds of a video.

This is also known as the thumb stop rate. This is one of the most important secondary metrics to track because it has a huge impact on the number of people who stick around on your ad and eventually convert. Next up is the hold rate.

Now this is a percentage of people who watched up to 15 seconds of your video. While the hook rate determines the clickbait potential of your video, the hold rate determines whether or not your follow-up visuals and messaging is keeping the right people around. Then we have the unique outbound click-through rate.

Now this is the percentage of people who clicked on your CTA and went directly to the website. We opt to use this metric as opposed to link click-through rate because this singles out individual users who clicked on your CTA button and actually got to the website. Ultimately, this metric reveals how effective your targeting and creating creatives are in tandem.

Additionally, I also like to look at conversion rates or purchases over link clicks. Now this metric shows the percentage of people who convert after the click. Typically, we're tracking this as a primary metric across the entire account performance, but I also really like to check it out on an ad by ad basis too.

Now post shares is kind of a random one that I don't optimize for in any way. But I think it's really interesting to see how many people actually share the ad. Now some final thoughts for new brands, because I know a lot of you guys are just starting out being media buyers.

So you're working with newer brands that maybe don't have as high of an ad spend, or you yourself are a business owner who are running ads for your own business, which is exactly how I got my start. So that's really exciting. So there are a few downsides to this testing methodology, and it really is probably best suited to brands who are spending more than 20k per month on Facebook ads.

And this is so that you can ensure that you're getting enough ad spend through the ads themselves. With that said, I have used this testing methodology for brands that are spending 5 to 10k. And results worked well, but I do find that just actually dripping in the new creatives into your core campaigns tends to have the best performance across the board.

If you're interested in learning more about that, or seeing a video about that, be sure to let me know in the comments below. Additionally, when faced with a new brand or a new pixel, I get this question all the time, should I start off with creative testing or audience testing? And my answer for that is to start off with creative testing.

And this is so that you can actually begin to find those initial wins at the ad level. And once you figure out what types of creatives are starting to work, then you can take those best performing creatives and start launching audience testing campaigns or ad sets. And that is it.

If you guys have any more questions about the creative testing methodology that we use at thesis, be sure to hit me up in the comments below. I'm very active there. And I will see you guys next Sunday.

Later. Bye.