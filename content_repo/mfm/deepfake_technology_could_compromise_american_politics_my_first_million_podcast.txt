But this is also like part of this deepfakes, like this deepfakes problem. And so if you don't know what a deepfake is, a deepfake is basically people can now use technology to make a video look real. You can make anything look, so for example, what they do is they'll take Obama.

Obama's talking. Yeah. But I can just say any sentence.

I could just be like, hey, my name's Barack, and I'm the worst president of all time. And it'll make it look like his lips are moving to my sentence because they basically feed in a bunch of footage of Obama talking normally. And it basically, the algorithm learns, okay, here's how Obama's face moves when he says certain things.

And then when I can input any audio, it'll make it look like he's doing it. And now, right now, the technology is where it looks a little off, but it's not that far off. Like if I showed it to my aunt, it'll leave it at all.

Right. But it's just getting better month over month. So they just showed a, like a paper just came out with a new one.

And it's literally just taking a still photo. So they took the Mona Lisa and then they do the same thing. I can go say a sentence and it will make the Mona Lisa say it.

Like the voice is that good. And so like, it's pretty crazy that they can just do that off a still photo. You don't even need like hours and hours of raw footage of the person in order to do this now.

So I downloaded an app that does that with the Mona Lisa. Which app is that? Do you remember?

Yeah. It's amazing. I'll tell you what it's called.

It's really neat. And I think that everyone should download this stuff. I paid $5 for it.

It's called. No, I don't remember the name of it. Okay.

No worries. But what do you think of this whole deepfakes thing? Because it's a big problem.

Like your sister's a lawyer and I think she was, she was like a DA before, right? Or something like that? Yeah.

Well, she works for the DA. Public defender. And then she's a public defender in the Bronx.

And people. Yeah. Now it's like, if you can't trust what you see, right?

If, if, if audio, like we already have photos that could be Photoshopped. Audio now that can be like Live Bird, I think is the name of it. Where basically you just, you just talk.

You can, you, you can train, you can train this system to where you can just type anything and it'll say it in your voice or it'll say it in Donald Trump's voice. So if you can't believe what you see, what you hear or what's in video, then how does that like mess up the world? Like fake news is going to become a bigger problem.

Yeah. It's going to become a problem. But why are we defaulting to that?

Why are we worrying about? I mean, yeah, it's going to be a problem. But also let's think about all the amazing things.

Like, like act, like there's going to be fake actors. Like my dance, like your dance video. Yeah.

Like it's going to be on that. It's going to make movies amazing. Like, like Brad Pitt won't exist.

It'll be, it'll, well, it will be, it'll be AJ Pitt and it will be some fake person. And I think that's awesome. So here's a future world.

You literally are just looking at your computer. And instead, like today, if you want to make a movie or an animation, you have to like be an animator or a movie editor or an artist to be able to draw. You're just going to be able to say things and it's going to be able to create what you're, what you're saying.

So you're going to be able to say, okay, what if Donald Trump was talking to, you know, whatever, Barack Obama, and then Donald jumped in the air and he said this, and then literally it's just going to be able to animate. That's how, that's how far this is going to go. And that's going to unlock the ability to create stuff to people who don't have the technical skills to do it today.

Yeah. I mean, I'm, but I'm not too worried about how they're going to regulate it because like if you have a political ad now, it like anyone could run an ad on Facebook or on TV. And it says Donald Trump, I'm Donald Trump and I hate Mexicans.

Anyone could do that now. But I mean, like literally the tech, like you could buy ads, but there are ways to say like, well, no, that's not allowed. And it has to say like, I'm Donald Trump and I support this.

You know what I mean? Like there are ways to like, like if you run a TV, a political TV ad right now, it has to say who pays for it. Sure.

It has to say, is this, but it doesn't have to be, it doesn't have to be a political thing, right? This could, this can go for anyone. Someone in high school can just bully somebody else.

Like right now, the thing that deep fakes got famous because people were putting other people's faces on porn actors and you could make it look like I could take a photo of your mom and I could put it on, put it into a porno and it would look like your mom's in the porno. And people were doing this with celebrities. People were doing this with their classmates.

And you know, the Reddit, the subreddit for this got huge is r slash deepfakes. R slash deepfakes. And so this is how it got popular because it was like this incredible bullying tool.

Like forget about political ads. It's just like slander left and right. Did it, did it get taken down that subreddit?

No, I think it's still up. I don't know. I haven't been in a long time, but when it first blew up, you know, it got really popular because people were, you know, if, oh yeah, it is banned from Reddit now because it was like, it was becoming a problem.

Oh yeah. But that, that didn't stop the actual, the actual problem. That just stopped the subreddit where people were sharing it.

So, so I think this is going to be a massive, massive problem for evidence. I think it's going to become a massive problem for just like slander and, you know, spin. Like I could, I could take a video of you and make it say that you hate Mexican people and it's not Donald Trump.

And, and people would believe it because they would see your face and they would say, I saw him say that. Yeah. Yeah.

I don't know. And so I'll tell you what some people are trying to do to combat this. There's a whole bunch of programmers who programmers in general are, have a high bar for the truth.

They really want things to be true, logical, solvable, reliable. And so when you take away evidence, evidence, which used to be a source of truth, it's like, well, you're saying this, but I have video of you doing that thing. So that's evidence.

But now it's like, well, this video could be anything. This video could be fake. So they're trying to solve this in the way that they think it can be solved.

So the theory is anything you make to sort of validate stuff, the con artist will always be one step ahead. And there's too big of a payoff to be able to fake this stuff. Well, it's like a counterfeiter.

And so you're always a cat and mouse game, which is not a winning solution. So the solution that they believe is that the phone makers themselves, the device makers themselves will need to put a cryptographic seal on the video when it's taken. And it's like a tamper proof seal like we have with medicine or whatever, where it's like, if this seal is broken, that means this video has been edited in some way.

And so at some point, people will only trust videos or photos that have this cryptographic seal on them that says this has not been edited because it's on the device itself from when it got captured. It gets instantly implemented on. And so- Who's working on that?

Well, the problem is a startup can't do this, right? Because it's actually Apple that needs to implement it. It's actually Samsung that needs to implement this.

And so luckily, Apple's a pretty privacy conscious company. And they know that if their tools are being used for evil, they'd usually actually do stuff about it. So hopefully, Apple is working on this.

But I have a friend who was doing a startup trying to do this. And he was running into this problem, which is like, look, the people who need to do this are all the camera makers. The security camera itself needs to do this.

And that's the only decent solution. Now, some technical people will say, oh, there's nothing that's actually tamper proof. You can still get around it.

Sure, sure, sure. But it's better. And they are right.

But there are still things where it mostly works. Right. Exactly.

So I think that's the way the world is going to work later, where you're going to need to see this little icon that says, it's the gluten-free icon. It's the- Organic. It's organic icon.

It's going to be, this is legitimate. And there's going to be a legitimacy icon on any photo or video that you show. The icon business, that's a good business.

You know J.D. Power? Well, I guess it's huge.

I would have to keep this stuff. Like, whatever. I could keep this stuff right.

And I don't know. Take it on a tall topic. You know L.D%.

Yeah. However, one of the ones who are playing in chat, you know, было like