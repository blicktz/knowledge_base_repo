I've spent more than $150 million on Facebook ads, working with more than 3,000 clients worldwide. And in this video, I'm going to show you how I test stuff within those Facebook ad accounts. Testing is incredibly important if you want your Facebook ads to produce fantastic results, but most Facebook advertisers do it wrong.

Once you've been through this video, you'll know how to test Facebook ads the right way. Okay, so the first point that I want to make is to give things more time. So Meta's algorithm works better when it has more data.

Your campaigns perform better when you have more conversion data, when Meta is better able to work out how to get new results that you want to get, that you've told Meta that you want to get by selecting your campaign objective and your performance goal. And a lot of advertisers just don't give things long enough when they're testing new ads or new creative or new headlines or new offers or whatever it is they're testing. If you just give things a couple of days, that's likely not long enough.

Now, a sort of default time period that I would advise is about seven to 10 days, but you need to be aware that you may need to adjust that and potentially adjust that significantly depending on your conversion volume, which is obviously going to be a function of your cost per conversion and your budget. So some businesses are going to generate 2000 purchases a day. Well, you can test things really quickly.

It might literally take you a few hours to work out whether or not something's going to work well on that. Now, we typically leave it a little bit longer than that. We might give that a couple of days, some new ads, for example, in that scenario.

But the data means you're going to see what's your cost per purchase, what's your return on ad spend, how is it in comparison to what you've run previously, how is it in comparison to your benchmarks that you've set, your breakeven numbers, your target numbers, things like that really fast. On the other side of things, if you're generating five conversions a week, well, it might take you weeks, maybe longer to find out if new ads are performing because it's just going to take you that much longer to get enough data to know, okay, this number is now representative of what we can expect from this ad in terms of cost per conversion, in terms of return on ad spend. Is it a good performer?

Is it not? Do we need to turn it off? Can we leave it running?

Can we scale it? It's going to take a lot longer. It's also a function, how long you need to leave things, of how far off your target you are.

So I just talked about your target. Let's say you're looking to generate a cost per purchase of $30 as an absolute maximum. Well, if you're at $35, you can give things, let's say within a seven-day window, you could give that some more time, see if Meta can work it out, maybe make some other improvements to see if it can come beneath your target.

If, however, your cost per purchase is $60, you're double where you need to be, you can make the call earlier to go, okay, this ad, we tested it, didn't work out as well. That's fine. These things happen.

Let's move on to something else. So one of the things I would recommend is have a play about with a statistical significance calculator, free online tools, but you can go in and you can enter in your data and compare, for example, a new ad versus a previous one that you've run. And the calculator will tell you to what degree of significance, if there even is a level of significance achieved statistically, that your new ad has outperformed or underperformed your old ad.

And when you enter in relatively low numbers, as in too early, when you don't have much data, you'll often see a statistical significance calculator will say, not enough data yet. Can't draw conclusions from this. And that's really useful for you to get a sense of as a Facebook advertiser, go, okay, this needs more time.

Whereas other times you'll enter in your numbers and they'll go, you know, to a 85%, to a 90% confidence level. We know that ad A is likely to outperform ad B or vice versa. If in doubt, be patient.

I think it's better to leave a test running a bit too long and potentially waste a bit of money on an ad that doesn't quite perform as well as another than have the alternative where you don't leave a test running long enough, which means you turn off something thinking, ah, it didn't work when actually it just needed more time. And that could have been something that produced results for you for weeks, months, maybe even sometimes years, you know, you have a creator that performs well enough. You can, you can sometimes run that for years depending on the market.

So better to leave things a little bit longer than to cut them too short. Point number two is quality over quantity. Now, a lot of people, most people in this space have heard tests are success.

You need to test a lot to succeed. And there's absolutely some truth to that. But I have seen a lot of advertisers, particularly in the last few years, overdoing it.

They're testing too much at once. Ideally, you would isolate the variable that you want to test and test that in isolation. So you don't want to be testing headlines and ad creative and ad format and offer all at the same time.

Because if you do that, and let's say one ad works really well, it's really difficult to work out, did that perform really well because of the creative or because of the headline or because it was an image instead of a video or vice versa, or because of, you know, there's too many things going on for you to be able to know why that one performed well. Whereas if you try and keep things as consistent as possible elsewhere and say like, right, we're going to test to start with ad format. So we're going to test images versus video.

We're going to have the copy remain the same, the headlines. We're going to have the offer remain the same. We're just going to test that to see which one is better.

And once you've run that experiment, you've got concrete data, you then move on to testing another element. So you absolutely want to test all those things, but just not at the same time, you need to be able to draw conclusions. And yeah, it's just something I see all the time where you test too many things, you go, well, we don't know why that performed well, why that performed poorly.

It's there's too much going on. I'd also be careful creating too much. A lot of people have these ideas around, oh, as part of my advantage plus campaign, I need to have 20 creatives, I have to have 20 creatives, otherwise there's no point running it.

Not true at all. And what they'll do is they'll maybe have four, five, six good ideas for ads, and the rest are just sort of fluff and variation. They just, it's like a tick-bock exercise.

They just think, oh, I just need to do this to get these out there to be able to hit this magical number for testing. I'd much rather you focused on creating four really good creatives as opposed to 20 mediocre ones. There's always constraints on resources for producing creative, whether it's time, money, both, et cetera.

So yeah, so don't overdo it when it comes to that sort of thing. You also need to be aware of your budget, how that factors in. The more you spend and the more conversions you generate in a shorter time period, the more you can test because Meta's able to dedicate budget to the various elements and give it some time and see how it performs.

If you're generating very few conversions and you go and throw 20 creatives into an Advantage Plus campaign, how is Meta going to optimize that properly? They're really going to struggle. So you do want to be careful there.

Not saying don't test, not saying don't test everything, but don't do it all at once and don't do too much at any one time. In a second, I'll get to some more tips and techniques when it comes to testing Facebook ads. Before I do, I want to quickly let you know about our done for you Facebook and Instagram advertising services.

So my company can create, manage and optimize your Facebook and Instagram ads for you. We can take that workload off your hands or take it away from an agency you might be working with who aren't producing the results that you're looking for and will almost certainly be able to help you get significantly better results because we do this all the time. We've spent tens of millions on the platform.

We've worked in every industry you can think of and we have a lot of experience and a great track record. So if that sounds interesting, there is a link in the description below. You can click on that, come through to a page on our website, book in a free call with one of my team members.

If you want to do that, go ahead. Hopefully we get a chance to work together. Number three is to plan first.

So before you run any test, I think it's really important to think about what is it that you're trying to work out? What's your hypothesis effectively? You need a plan ahead of time as opposed to just, well, I just heard you should test so I just kind of created different stuff and now I don't really know what conclusions I can draw from the test that I've run, right?

So an example would be, I have the hypothesis that UGC ads will outperform the ads that we're currently running. Now if that's your hypothesis, that's the test that you're going to run, it's nice and specific and it directs how you're going to treat that test. So what you would then need to do is create a number of UGC ads to test against your existing ads.

Now, when you frame the test that way, you come to the right conclusion and you do the right thing because for example, you wouldn't want to test one UGC ad to test that hypothesis because if that doesn't work, it might not be that UGC either outperforms or doesn't outperform your regular ads. It's just that maybe that ad didn't work or maybe that ad did work. So you need a range of UGC ads, perhaps like different people involved, different styles and test that.

So you get like a batch of those against a batch of other style of ads and then you can see which one performs best and you get a more sense of an overall performance of that. So as you're going through and you're testing these various elements, you want to first think about, okay, what am I going to test next? What do I think is likely to move the needle most?

What's most important? And then approach it through that idea as opposed to just sort of random testing. And that leads me on to my next point, which is to start with the big needle movers.

So testing primary text, for example, is valuable. It's something that we will do, but it's not something you want to do until you've worked out the more important stuff, like what's the best style of ad for your offer? Is it UGC?

Is it founder led? Is it a product demonstration type ad? Is it an influencer based ad?

Is it video? Is it carousel? Is it single image?

These things are much more important to work out. They are bigger needle movers than changing some of the wording and the copy in your primary text. So start with the big stuff, then move on to the smaller things.

And I've got a sort of a list of prioritization I'm going to run through now of how we would typically look at this when working with clients. So the first thing is going to be the product itself or the service, right? Like which product or service should you sell?

You might already have data on that. You might know what are your best sellers, in which case you can focus on those. Maybe that is something though that you need to test.

You're like, I don't actually know which of my best sellers or which of my products or services sell best to this meta ads audience. Maybe we need to do some experimentation around that as a very top level thing before we then go ahead and move on. After that, you move on to the offer.

Can you test making it more urgent or scarce or more appealing? Can you add in guarantees to offer some risk reversal around the offer itself? This is especially important if you don't have large warm audiences where the offer and the power of the offer is going to be less important, which is obviously the majority of businesses.

After the offer, we're going to move on to creative. Which format is going to perform best? Do we need to test that?

Do we need to test carousel versus image versus video? And then which style? I ran through a number of options there.

Is it UGC? Is it influencer? Is it product demo?

Is it founder led? Is it all sorts of different things? Okay.

Then we move on to headline after that primary text, then the CTA button, then the description would be like a typical list. Now there might be occasions where we decide, actually, I think we need to test this ahead of this because it could be more important. And when you're changing and testing things like the offer, the headlines are naturally going to change with them because the ad needs to make sense, right?

And maybe the creative to some extent too. So it's not as perfect as that set out, but that's how we think through it. Like, okay, we've got the offer sorted.

We know this style of ad works really well for this business. This is the sort of creative we're going with. Now we can test different headlines variations with all those things that are more important being fixed.

That's something that we would look to do and think through the process of testing. Now, what that often means as you're running through that list is that you're starting with the hard stuff first. And that's why people don't often do this.

It's really easy to test a new call to action button or tweak a bit of copy on a primary text. Like that might take you a few minutes, especially using chat GPT or something to help you write the primary text variations, or even just Meta's own AI text generation from within the ad creation process, right? But recrafting an offer, changing your product, producing high quality video creative with influencers, these are hard things.

That's why people don't do it. But that's also why that's where the rewards are, because people don't want to do the hard things. Your competitors aren't doing the hard things.

If you can do the hard things, then you're far more likely to have a competitive advantage and get the reward. So it's easy to do. But my recommendation on testing would be don't take the easy route, take the route that leads to better performance, so much better over the long run.

And when you are testing, it is absolutely essential that you have accurate data so you can see how your Facebook ads are performing and optimize them properly. Hyros is the best tracking and attribution software for Facebook and Instagram ads. It's a software that all the big players, including myself, use.

I use it to track the results that I generate from my own ads. I also use it for all my organic content as well, which is really useful. So if someone comes through and books a call for our done-for-you services, for example, I can see exactly where they came from with a lot of accuracy, and that allows me to make decisions in my business, which is really, really helpful.

And let me quickly show you how important this is. So I'm in my Hyros dashboard. I've isolated one of our meta ad campaigns, and I've created a report for it so we can see the data, and you can take a look at this.

So this campaign spent just under £2,000, generated £15,000 in revenue. But here's the really important bit, reported versus revenue. And in this column, we can see £12,000.

Now, what that means is that out of the £15,000 that this campaign has actually generated, Hyros has been able to report on the full £15,000, £12,000 of which Meta was not originally able to see. So if we're just looking within our meta ad account, not using Hyros, we would have seen that £3,000 of revenue was generated from this campaign. And that is not true at all.

That is a much, much lower ROAS number than the actual £15,000 of revenue that was generated. For a number of years now, Meta's ad tracking capabilities have not been as accurate as they once were, because all sorts of privacy regulations have come into effect, and it's only heading that way. It's only becoming more and more difficult to get accurate data and track our results as we need to be able to do.

This is a relatively extreme case, and the reason why is because Meta has a seven-day maximum attribution window. And this is an ad campaign that's retargeting for one of our services that people usually take a lot longer than seven days to make a decision on. So if someone clicks on an ad, they often won't convert within seven days.

Meta's simply not able to see that data, and that's where Hyros comes in. The important thing is that you need accurate data, and Hyros takes care of that. So if you want to get better results from your Facebook ads without actually having to touch your ads, go ahead and sign up to Hyros.

There is a link in the video description. Point number six is to take charge. So if you've spent money, time producing ad creative, for example, that you think's really good, you're like, I think this has got a really good chance of performing really well.

And then you introduce it into an existing campaign alongside previous ads, and Meta just gives it no budget, or you've created a lot in a whole batch, you've introduced that all into a new campaign, and Meta's putting all the budget into just one or two bits of creative, not giving anything else any love. Don't always assume that that's correct. Sometimes Meta's accurately predicted that the other stuff won't perform as well as the stuff it's giving budget to, but it's not always the case, particularly if you're introducing new ads alongside existing ones, because the social proof, the shares, likes, comments, interactions that the previous ads have can make a big difference.

It can help those ads outperform the new ones, even if the new ones do have potential. Meta sees that, they use that as a signal, and they don't give the new ones any budget. So in that scenario, which we run into all the time, we will often create a separate testing campaign where we're going to basically force Meta to spend money on this new stuff that we think can work really well.

So in that testing campaign, we might use ad set budgets, and like I said, we'll force Meta to spend on new ad creative, or whatever it is that we're testing. Now when you do that, you need to be aware that you may well see a drop in your return on ad spend, often temporary, or a drop in, or a worsening in your cost per conversion, an increase in your cost per conversion. Because those existing ads with the social proof, those are like over the next two weeks, let's say, they're going to perform better than some of the budget, most of the budget being spent on them anyway, and then some of the budget being spent on new ads to test.

But I think that over the long run, finding out how those new ads can perform is more important. It's really, really valuable. Because if you don't do that, you get into this situation, so many Facebook advertisers are here, where they found a winning ad, and they're running it.

Every time they try and introduce new ads alongside it, it doesn't really get any budget, it just seems pointless. They sort of stop testing, keep introducing new ads, they keep running this good performer. But slowly over time, it fatigues, and results just get worse, and worse, and worse.

And you need to take charge of that situation, be like, no, no, no, I'm going to force some ad spend on these new ads, I think they can perform really well. And knowing, okay, you know what, this one's still our winner. But once it fatigues, we've got these ones waiting in the wings to go.

That's really important, particularly over the long run. Number seven is to find things to test. So I'd recommend that you take a look at what your competitors are doing.

And not just from an advertising standpoint, but look at how people interact and talk about their products or services. So take a look at their reviews, right? What parts of what they do do people love and give five-star reviews for?

What do they mention their five-star reviews? And then conversely, what parts about what your competitors do do people hate? Do they give one-star reviews for?

People might care about things or want to avoid things that you didn't expect and think, oh yeah, we do that. People don't like that. Let's stop doing that.

Or, okay, we don't do enough of that. People seem to love that in our competitor reviews. Let's do more of that.

And then obviously that impacts the offer, the product, the service that you offer, which then in turn impacts what you might be able to test within your advertising. Be like, okay, in these five-star reviews, people keep mentioning this thing. We do that, but we don't feature that prominently in our ad copy and creative.

Let's make that front and center. Let's test that as an angle in order to get people to be interested. So not something many advertisers do, but very easy to do and can give you loads of ideas for ads.

And if there are things that people leave one-star reviews for and really don't like, and they mention, you know, in their one-star review, they mention, oh, they do this thing I don't like. You can take that as your angle within your ad, within your creative and copy. You can say, by the way, we don't do this.

You know, we don't, we guarantee our delivery times. We make sure you're not left waiting. We have good customer service that you can actually access.

It's not, you know, fighting your way through a call center system of, you know, nine buttons and half an hour and all that jazz. Like you can, you can focus on those angles as well. You can also do the same thing for industries and niches that are similar to your business, but not within your exact space.

I actually quite like this. It's a good way of finding inspiration for new ads as well, because you can sometimes find something that people in your space aren't doing, but looks like it's working really well in another space that's similar. And you think, you know what?

I can bring that in to my industry. We can gain a massive competitive advantage. And that could be a massive win for us when it comes to return on ad spend.

So I've given this example before, I've created a whole video around an industry that never used to have a finance option. We introduced a finance option, which obviously was being done in other industries a few years ago. And the ads just, I mean, absolutely crushed because no one else in that space was doing it.

And it led to amazing results. I'd also recommend that you keep an eye out when you're using social media yourself, which we probably all do more than, you know, we plan to and we'd like to. Even if we plan to go on the business purposes, we get sort of hijacked, don't we?

But if you are doing that, from an ad standpoint, what grabs your attention? What might you be able to use? Just keep an open mind, have a little folder saved.

We are, okay, I'm going to quickly pop this in here. I think this was a really good ad. This grabbed my attention.

I haven't seen people in my space do this sort of thing. Maybe I can replicate it. Point number eight, never stop testing.

It's really easy once you find something that works well to just stop testing. Be like, aha, I found the ad. I can run with this.

Not a good idea though, because everything fatigues. Facebook and Instagram advertising is interruption advertising. Unless your target audience is changing really, really quickly, which does happen in some circumstances, like if it's based around a life event and the audience is auto-refreshing every few months or something like that, these things do happen, but they're rare.

And even in those scenarios, if you try and scale, you're still going to run into fatigue issues because you're going to run through that audience before it manages to refresh with the one creative. Which means that if you understand that things are going to fatigue, you have to have other ads, other bits of creative ready to go. You have to have other options, other offers, maybe even other products and services, depending on your industry and what it is that you do.

You can't just sit there and go, this is brilliant. There's also an element of the platform changes. So as the platform evolves, if you just go, okay, I've got an ad, I'm going to run this for the next three years, everything's going to be fine.

What happens if in a year there's a new feature that would make that ad much better and you're not taking advantage of it? Or you're offering something that works really well. Oh, you've got a guarantee.

Most of your competitors do. You don't. You've got some sort of special offer.

And then your competitors cotton on, they copy you, they start doing the same thing. Well, now your offer, your guarantee that made you really attractive is not as important anymore. And you need to go again and develop something new and change it up.

These sorts of things happen. I think a lot of people, they might be aware of the platform changes and they think, okay, I'll need to tweak some settings. But actually what the other things that happen as well, that does make a difference.

What your competitors are doing in the marketplace absolutely makes a difference because most people who are seeing your ads are also seeing ads from your competitors. And you really have to be aware of that when you're putting your Facebook and Instagram ads together and thinking, okay, if someone's scrolling on Instagram for 20 minutes, they're going to see my ads and my competitors. I need to not just make my product, my service, my whatever appealing.

I need to make it more appealing than this competitor in order to get the best results possible. So I don't think many people think about that, but that's something I would definitely recommend. Now, one thing I haven't talked much about in this video is, you know, when you're testing, should you be doing it through Advantage Plus campaigns, through manual campaigns, all that sort of stuff.

And I imagine some of you going through this thinking, yeah, what's the structure? And I haven't done that on purpose because Meta, as I'm recording this, is in the process of completely changing Facebook ad campaign structure. We're effectively all going to be using a variation of Advantage Plus campaigns, but there's significant nuance to that.

There's lots of options and flexibility. Anyway, I've created a whole video about the new changes. I've been able to get early access and a sneak peek into what's happening with these.

And I've done a video walking it through. You can go ahead and check it out here. This is one of the biggest changes to happen to Facebook ads in years, probably the biggest since Advantage Plus came along almost three years ago.

So if you haven't already, definitely go ahead and check this out. Thanks guys. Bye for now.