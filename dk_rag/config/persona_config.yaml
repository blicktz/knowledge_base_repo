# Configuration for Virtual Influencer Persona Agent - Multi-Tenant Architecture

# LLM Configuration for Persona Extraction
llm:
  provider: "openrouter"  # openrouter, openai, anthropic
  config:
    api_key: "${OPENROUTER_API_KEY}"
    model: "openrouter/openai/gpt-5"
    fallback_models:
      - "openrouter/openai/gpt-5-mini"
      - "openrouter/google/gemini-2.5-pro"
    temperature: 1.0  # GPT-5 only supports temperature=1
    max_tokens: 4000
    timeout: 60
    num_retries: 3
    site_url: "https://persona-agent.com"
    app_name: "Virtual Influencer Persona Agent"

# Vector Database Configuration
vector_db:
  provider: "chromadb"
  config:
    persist_directory: "./data/storage/chroma_db"
    collection_name: "script_collection"
    distance_metric: "cosine"
    embedding_model: "sentence-transformers/all-mpnet-base-v2"
    chunk_size: 500
    chunk_overlap: 100

# Statistical Analysis Configuration
statistical_analysis:
  spacy:
    model: "en_core_web_sm"  # Download with: python -m spacy download en_core_web_sm
    pipeline_components:
      - "tok2vec"
      - "tagger" 
      - "parser"
      - "ner"
      - "attribute_ruler"
      - "lemmatizer"
    max_length: 1000000  # Maximum text length for processing
    
  nltk:
    collocations:
      window_size: 3
      min_frequency: 3
      significance_threshold: 5.0
    ngrams:
      min_n: 2
      max_n: 4
      min_frequency: 2
    stopwords: "english"
    
  keywords:
    max_features: 1000
    min_frequency: 2
    max_frequency: 0.8
    ngram_range: [1, 3]

# Persona Extraction Configuration
persona_extraction:
  mental_models:
    min_confidence: 0.7
    max_models: 50
    min_steps: 2
    max_steps: 10
    
  core_beliefs:
    min_confidence: 0.6
    max_beliefs: 100
    min_frequency: 1
    
  linguistic_style:
    max_catchphrases: 50
    max_vocabulary: 100
    min_phrase_length: 3
    
  quality_thresholds:
    min_total_words: 1000
    min_documents: 1
    min_unique_sentences: 50

# Data Processing Configuration
data_processing:
  chunk_strategy: "semantic"  # semantic, fixed, sentence
  chunk_size: 500
  chunk_overlap: 100
  min_chunk_length: 100
  max_chunk_length: 1000
  
  filters:
    min_sentence_length: 10
    max_sentence_length: 500
    remove_urls: true
    remove_emails: true
    remove_phone_numbers: true
    
  preprocessing:
    normalize_whitespace: true
    remove_empty_lines: true
    fix_encoding: true

# Storage Configuration
storage:
  # Base directories - personas will be created under these
  artifacts_dir: "./data/storage/artifacts"
  vector_db_dir: "./data/storage/chroma_db"
  cache_dir: "./data/storage/cache"
  logs_dir: "./logs"
  
  # Multi-tenant settings
  multi_tenant:
    enabled: true
    personas_base_dir: "./data/storage/personas"
    isolation_mode: "strict"  # strict = completely separate stores, shared = shared embeddings
  
  backup:
    enabled: true
    frequency: "daily"  # daily, weekly, manual
    retention_days: 30
    
  compression:
    enabled: true
    algorithm: "gzip"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    console:
      enabled: true
      level: "INFO"
    file:
      enabled: true
      level: "DEBUG"
      filename: "persona_agent.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
      
# Device Configuration for GPU Acceleration
device:
  # Device preference: auto, mps, cuda, cpu
  # auto = automatically detect best available device
  preferred_device: "auto"
  
  # Force CPU for specific libraries (debugging)
  force_cpu:
    spacy: false
    sentence_transformers: false
    
  # Memory management
  memory_optimization:
    clear_cache_between_operations: true
    batch_processing: true

# Performance Configuration
performance:
  max_workers: 4
  batch_size: 100
  cache_size: 1000
  memory_limit_mb: 2048
  
  optimization:
    use_multiprocessing: true
    parallel_extraction: true
    cache_embeddings: true
    lazy_loading: true

# Validation Configuration
validation:
  strict_mode: false
  auto_fix: true
  
  required_fields:
    persona_constitution:
      - "linguistic_style"
      - "statistical_report"
      - "extraction_metadata"
      
  quality_checks:
    min_mental_models: 3
    min_core_beliefs: 5
    min_catchphrases: 3
    min_vocabulary_terms: 10

# Map-Reduce Extraction Configuration
map_reduce_extraction:
  enabled: true  # Enable map-reduce for large corpus processing
  
  # LLM configuration for map-reduce phases
  llm_provider: "litellm"
  llm_model: "gemini/gemini-2.0-flash"  # Cost-effective for batch processing
  
  # Alternative models for different phases
  map_phase_model: "gemini/gemini-2.0-flash"  # For processing batches (uses GEMINI_API_KEY)
  reduce_phase_model: "gemini/gemini-2.5-flash"  # For consolidation (uses GEMINI_API_KEY)
  
  # Batch processing parameters
  batch_size: 15  # Number of documents per batch
  max_tokens_per_batch: 500000  # Max tokens to send per batch (leveraging Gemini 2.0 Flash 1M context)
  parallel_batches: 1  # Reduced for Gemini rate limits (10 requests/min)
  
  # Caching configuration
  cache_batch_results: true  # Save individual batch results
  resume_from_cache: true  # Resume from cached batches if interrupted
  cache_compression: true  # Compress cached batch results
  cache_ttl_hours: 1000000  # Cache time-to-live (indefinite)
  
  # Processing parameters
  max_retries: 2  # Retries for failed batches
  retry_delay_seconds: 10  # Delay between retries (increased for rate limits)
  timeout_seconds: 120  # Timeout per batch
  
  # Consolidation strategies
  mental_models:
    consolidation_strategy: "frequency_weighted"  # frequency_weighted, confidence_based, hybrid
    min_frequency: 2  # Minimum appearances across batches to include
    top_k: 50  # Keep top K models after consolidation
  
  core_beliefs:
    consolidation_strategy: "frequency_weighted"
    min_frequency: 2
    top_k: 100  # Keep top K beliefs after consolidation
  
  # Progress tracking
  show_progress: true  # Show progress bars
  save_intermediate: true  # Save after each batch
  
# Development Configuration
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  
  testing:
    use_sample_data: false
    sample_size: 100
    mock_llm_calls: false