# Configuration for Virtual Influencer Persona Agent - Phase 1

# LLM Configuration for Persona Extraction
llm:
  provider: "openrouter"  # openrouter, openai, anthropic
  config:
    api_key: "${OPENROUTER_API_KEY}"
    model: "openrouter/openai/gpt-5"
    fallback_models:
      - "openrouter/google/gemini-2.5-pro"
    temperature: 0.1  # Low temperature for consistent extractions
    max_tokens: 4000
    timeout: 60
    num_retries: 3
    site_url: "https://persona-agent.com"
    app_name: "Virtual Influencer Persona Agent"

# Vector Database Configuration
vector_db:
  provider: "chromadb"
  config:
    persist_directory: "./data/storage/chroma_db"
    collection_name: "script_collection"
    distance_metric: "cosine"
    embedding_model: "sentence-transformers/all-mpnet-base-v2"
    chunk_size: 500
    chunk_overlap: 100

# Statistical Analysis Configuration
statistical_analysis:
  spacy:
    model: "en_core_web_sm"  # Download with: python -m spacy download en_core_web_sm
    pipeline_components:
      - "tok2vec"
      - "tagger" 
      - "parser"
      - "ner"
      - "attribute_ruler"
      - "lemmatizer"
    max_length: 1000000  # Maximum text length for processing
    
  nltk:
    collocations:
      window_size: 3
      min_frequency: 3
      significance_threshold: 5.0
    ngrams:
      min_n: 2
      max_n: 4
      min_frequency: 2
    stopwords: "english"
    
  keywords:
    max_features: 1000
    min_frequency: 2
    max_frequency: 0.8
    ngram_range: [1, 3]

# Persona Extraction Configuration
persona_extraction:
  mental_models:
    min_confidence: 0.7
    max_models: 50
    min_steps: 2
    max_steps: 10
    
  core_beliefs:
    min_confidence: 0.6
    max_beliefs: 100
    min_frequency: 1
    
  linguistic_style:
    max_catchphrases: 50
    max_vocabulary: 100
    min_phrase_length: 3
    
  quality_thresholds:
    min_total_words: 1000
    min_documents: 1
    min_unique_sentences: 50

# Data Processing Configuration
data_processing:
  chunk_strategy: "semantic"  # semantic, fixed, sentence
  chunk_size: 500
  chunk_overlap: 100
  min_chunk_length: 100
  max_chunk_length: 1000
  
  filters:
    min_sentence_length: 10
    max_sentence_length: 500
    remove_urls: true
    remove_emails: true
    remove_phone_numbers: true
    
  preprocessing:
    normalize_whitespace: true
    remove_empty_lines: true
    fix_encoding: true

# Storage Configuration
storage:
  artifacts_dir: "./data/storage/artifacts"
  vector_db_dir: "./data/storage/chroma_db"
  cache_dir: "./data/storage/cache"
  logs_dir: "./logs"
  
  backup:
    enabled: true
    frequency: "daily"  # daily, weekly, manual
    retention_days: 30
    
  compression:
    enabled: true
    algorithm: "gzip"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    console:
      enabled: true
      level: "INFO"
    file:
      enabled: true
      level: "DEBUG"
      filename: "persona_agent.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
      
# Performance Configuration
performance:
  max_workers: 4
  batch_size: 100
  cache_size: 1000
  memory_limit_mb: 2048
  
  optimization:
    use_multiprocessing: true
    parallel_extraction: true
    cache_embeddings: true
    lazy_loading: true

# Validation Configuration
validation:
  strict_mode: false
  auto_fix: true
  
  required_fields:
    persona_constitution:
      - "linguistic_style"
      - "statistical_report"
      - "extraction_metadata"
      
  quality_checks:
    min_mental_models: 3
    min_core_beliefs: 5
    min_catchphrases: 3
    min_vocabulary_terms: 10

# Development Configuration
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  
  testing:
    use_sample_data: false
    sample_size: 100
    mock_llm_calls: false