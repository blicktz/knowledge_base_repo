# Configuration for Virtual Influencer Persona Agent - Multi-Tenant Architecture

# LLM Configuration for Persona Extraction
llm:
  provider: "openrouter"  # openrouter, openai, anthropic
  config:
    api_key: "${OPENROUTER_API_KEY}"
    model: "openrouter/openai/gpt-5"
    fallback_models:
      - "openrouter/openai/gpt-5-mini"
      - "openrouter/google/gemini-2.5-pro"
    temperature: 1.0  # GPT-5 only supports temperature=1
    max_tokens: 4000
    timeout: 60
    num_retries: 3
    site_url: "https://persona-agent.com"
    app_name: "Virtual Influencer Persona Agent"


# Statistical Analysis Configuration
statistical_analysis:
  spacy:
    model: "en_core_web_sm"  # Download with: python -m spacy download en_core_web_sm
    pipeline_components:
      - "tok2vec"
      - "tagger" 
      - "parser"
      - "ner"
      - "attribute_ruler"
      - "lemmatizer"
    max_length: 1000000  # Maximum text length for processing
    
  nltk:
    collocations:
      window_size: 3
      min_frequency: 3
      significance_threshold: 5.0
    ngrams:
      min_n: 2
      max_n: 4
      min_frequency: 2
    stopwords: "english"
    
  keywords:
    max_features: 1000
    min_frequency: 2
    max_frequency: 0.8
    ngram_range: [1, 3]

# Persona Extraction Configuration
persona_extraction:
  mental_models:
    min_confidence: 0.7
    max_models: 100
    min_steps: 2
    max_steps: 10
    
  core_beliefs:
    min_confidence: 0.6
    max_beliefs: 100
    min_frequency: 1
    
  linguistic_style:
    max_catchphrases: 50
    max_vocabulary: 100
    min_phrase_length: 3
    
  quality_thresholds:
    min_total_words: 1000
    min_documents: 1
    min_unique_sentences: 50

# Data Processing Configuration
data_processing:
  chunk_strategy: "semantic"  # semantic, fixed, sentence
  chunk_size: 500
  chunk_overlap: 100
  min_chunk_length: 100
  max_chunk_length: 1000
  
  filters:
    min_sentence_length: 10
    max_sentence_length: 500
    remove_urls: true
    remove_emails: true
    remove_phone_numbers: true
    
  preprocessing:
    normalize_whitespace: true
    remove_empty_lines: true
    fix_encoding: true

# Storage Configuration
storage:
  # Base storage directory - all data will be stored under this path
  base_storage_dir: "/Volumes/J15/aicallgo_data/persona_data_base"
  logs_dir: "./logs"
  
  backup:
    enabled: true
    frequency: "daily"  # daily, weekly, manual
    retention_days: 30
    
  compression:
    enabled: true
    algorithm: "gzip"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    console:
      enabled: true
      level: "INFO"
    file:
      enabled: true
      level: "DEBUG"
      filename: "persona_agent.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
      
# Device Configuration for GPU Acceleration
device:
  # Device preference: auto, mps, cuda, cpu
  # auto = automatically detect best available device
  preferred_device: "auto"
  
  # Force CPU for specific libraries (debugging)
  force_cpu:
    spacy: false
    sentence_transformers: false
    
  # Memory management
  memory_optimization:
    clear_cache_between_operations: true
    batch_processing: true

# Performance Configuration
performance:
  max_workers: 4
  batch_size: 100
  cache_size: 1000
  memory_limit_mb: 2048
  
  optimization:
    use_multiprocessing: true
    parallel_extraction: true
    cache_embeddings: true
    lazy_loading: true

# Validation Configuration
validation:
  strict_mode: false
  auto_fix: true
  
  required_fields:
    persona_constitution:
      - "linguistic_style"
      - "statistical_report"
      - "extraction_metadata"
      
  quality_checks:
    min_mental_models: 3
    min_core_beliefs: 5
    min_catchphrases: 3
    min_vocabulary_terms: 10

# Map-Reduce Extraction Configuration
map_reduce_extraction:
  enabled: true  # Enable map-reduce for large corpus processing
  skip_reduce: true  # When true, output mapped results directly without reduce phase
  
  # LLM configuration for map-reduce phases
  llm_provider: "litellm"
  llm_model: "gemini/gemini-2.5-flash"  # Cost-effective for batch processing
  
  # Alternative models for different phases
  map_phase_model: "gemini/gemini-2.0-flash"  # For processing batches (uses GEMINI_API_KEY)
  reduce_phase_model: "gemini/gemini-2.5-pro"  # For consolidation (uses GEMINI_API_KEY)
  
  # Batch processing parameters
  batch_size: 5  # Number of documents per batch
  max_tokens_per_batch: 500000  # Max tokens to send per batch (leveraging Gemini 2.0 Flash 1M context)
  parallel_batches: 1  # Reduced for Gemini rate limits (10 requests/min)
  
  # Caching configuration
  cache_batch_results: true  # Save individual batch results
  resume_from_cache: true  # Resume from cached batches if interrupted
  cache_compression: true  # Compress cached batch results
  cache_ttl_hours: 1000000  # Cache time-to-live (indefinite)
  
  # Processing parameters
  max_retries: 2  # Retries for failed batches
  retry_delay_seconds: 10  # Delay between retries (increased for rate limits)
  timeout_seconds: 120  # Timeout per batch
  
  # Consolidation strategies
  mental_models:
    consolidation_strategy: "frequency_weighted"  # frequency_weighted, confidence_based, hybrid
    min_frequency: 1  # Minimum appearances across batches to include
    top_k: 100  # Keep top K models after consolidation
  
  core_beliefs:
    consolidation_strategy: "frequency_weighted"
    min_frequency: 2
    top_k: 100  # Keep top K beliefs after consolidation
  
  # Progress tracking
  show_progress: true  # Show progress bars
  save_intermediate: true  # Save after each batch
  
# Development Configuration
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  
  testing:
    use_sample_data: false
    sample_size: 100
    mock_llm_calls: false

# Phase 2 Advanced Retrieval Configuration
retrieval:
  enabled: true  # Enable Phase 2 advanced retrieval
  
  # HyDE (Hypothetical Document Embeddings)
  hyde:
    enabled: true
    prompt_template: "default"
    cache_size: 128
    cache_ttl_hours: 168  # 1 week
    auto_select_prompt: true
    
    # Dedicated LLM for fast hypothesis generation
    llm_provider: "litellm"
    llm_model: "gemini/gemini-2.0-flash"  # Fast model (uses GEMINI_API_KEY)
    temperature: 0.7
    max_tokens: 4000
    timeout_seconds: 30
    max_retries: 2
    
  # Hybrid Search (BM25 + Vector)
  hybrid_search:
    enabled: true
    bm25_weight: 0.4
    vector_weight: 0.6
    bm25_k1: 1.5
    bm25_b: 0.75
    retrieval_k_multiplier: 2
    use_rrf: false  # Use weighted fusion instead of RRF
    rrf_k: 60
    
  # Cross-Encoder Reranking
  reranking:
    enabled: true
    model: "mixedbread-ai/mxbai-rerank-large-v1"
    use_cohere: false  # Set to true to use Cohere API
    cohere_api_key: "${COHERE_API_KEY}"  # Optional Cohere API key
    cohere_model: "rerank-english-v3.0"
    top_k: 5
    batch_size: 32
    device: "auto"  # auto-detect GPU/CPU
    
  # Caching and Logging
  caching:
    enabled: true
    hyde_cache_size: 128
    rerank_cache_size: 256
    cache_ttl_hours: 168
    enable_compression: true
    log_llm_interactions: true
    log_performance_metrics: true
    
  # Storage Paths
  storage:
    base_storage_dir: "/Volumes/J15/aicallgo_data/persona_data_base"
    bm25_index_path: null  # Uses base_storage_dir/indexes/bm25 if null
    cache_dir: null  # Uses base_storage_dir/retrieval_cache if null
    
  # Pipeline Settings
  pipeline:
    default_k: 5  # Default number of final results
    default_retrieval_k: 25  # Default candidates before reranking
    enable_fallback: true  # Fallback to basic search on errors
    log_pipeline_execution: true
    parallel_processing: false

# Phase 3 Agent Configuration
agent:
  enabled: true
  
  # Memory Management Configuration
  memory:
    enabled: true
    max_tokens: 6000  # Maximum tokens for conversation history
    strategy: "last"  # Keep most recent messages
    include_system: true  # Always preserve system message
    start_on: "human"  # Ensure trimmed history starts with human message
    end_on: ["human", "tool"]  # Valid ending message types
  
  # LLM Logging Configuration
  llm_logging:
    enabled: true
    directory_name: "llm_logging"  # Will be placed under base_storage_dir/persona_id/
    save_prompts: true
    save_responses: true
    save_extracted: true
    include_metadata: true
    compression: true
    retention_days: 30  # Optional: auto-cleanup old logs
  
  # Query Analysis Tool Configuration
  query_analysis:
    llm_provider: "litellm"
    llm_model: "gemini/gemini-2.0-flash"  # Light task - fast model
    temperature: 0.3  # Lower temperature for structured extraction
    max_tokens: 1000
    timeout_seconds: 20
    max_retries: 1
    cache_results: true
    cache_ttl_hours: 24
    log_interactions: true
    
  # Synthesis Engine Configuration  
  synthesis:
    llm_provider: "litellm"
    llm_model: "gemini/gemini-2.5-pro"  # Heavy lifting - best model
    temperature: 0.7  # Higher temperature for creative synthesis
    max_tokens: 10000
    timeout_seconds: 300
    max_retries: 1
    use_chain_of_thought: true
    include_scratchpad: true
    log_interactions: true
    
  # Tool-specific Settings
  tools:
    # Query Analysis Tool (Light task - fast model)
    query_analysis:
      llm_provider: "litellm"
      llm_model: "gemini/gemini-2.0-flash"  # Light task - fast model
      temperature: 0.3  # Lower temperature for structured extraction
      max_tokens: 1000
      timeout_seconds: 20
      max_retries: 2
      cache_results: true
      cache_ttl_hours: 24
      log_interactions: true
    
    # Persona Data Tool
    persona_data:
      cache_session: true  # Cache per session to avoid repeated loading
      cache_ttl_minutes: 60
      auto_cleanup_temp_files: true
      
    # Mental Models Retriever Tool
    mental_models:
      k: 3  # Number of mental models to retrieve
      use_reranking: true
      min_confidence_score: 0.7
      log_retrievals: true
      
    # Core Beliefs Retriever Tool
    core_beliefs:
      k: 3  # Number of core beliefs to retrieve
      use_reranking: true
      min_confidence_score: 0.6
      include_evidence: true
      log_retrievals: true
      
    # Transcript Retriever Tool
    transcripts:
      k: 3  # Number of transcript chunks to retrieve
      retrieval_k: 25  # Candidates before reranking
      use_phase2_pipeline: true  # Use full Phase 2 pipeline
      log_retrievals: true
      
    # General Tool Settings
    parallel_execution: true
    timeout_seconds: 30
    fail_fast: true  # No fallback initially - fail immediately on error
    
  # LLM Interaction Logging
  logging:
    enabled: true
    save_prompts: true
    save_responses: true
    save_extracted: true
    include_metadata: true
    log_directory: "logging/llm_interactions"  # Under base_storage_dir/persona_id/
    
    # Component-specific log directories
    components:
      query_analysis: "query_analysis"
      mental_models: "mental_models"
      core_beliefs: "core_beliefs"
      transcripts: "transcripts"
      synthesis: "synthesis"
      
  # Performance Settings
  performance:
    enable_progress_logging: true
    log_timing_metrics: true
    batch_tool_execution: false  # Sequential for now
    max_concurrent_tools: 1  # Sequential execution
    
  # Error Handling (Simplified - No Fallbacks)
  error_handling:
    use_fallbacks: false  # Disabled initially
    throw_on_error: true  # Fail fast for debugging
    include_traceback: true  # Full error details in logs

# FastAPI Configuration  
api:
  enabled: true
  host: "0.0.0.0"
  port: 8000
  reload: false  # Set to true for development
  workers: 1
  
  # API Settings
  settings:
    title: "Persona Agent API"
    version: "1.0.0"
    docs_url: "/docs"
    redoc_url: "/redoc"
    
  # Rate Limiting
  rate_limiting:
    enabled: false  # Enable later for production
    requests_per_minute: 60
    
  # CORS Settings
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["GET", "POST"]
    allow_headers: ["*"]
    
  # Health Check
  health_check:
    enabled: true
    endpoint: "/health"
    include_version: true